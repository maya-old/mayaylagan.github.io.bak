---
title: 'Project 2: Modeling, Testing, and Predicting'
author: "SDS348"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
---

```{r setup, include=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

knitr::opts_chunk$set(echo = TRUE, eval = TRUE,fig.align="center",warning=FALSE,message=FALSE,fig.width=8, fig.height=5, linewidth=60)
options(tibble.width = 100,width = 100)

```

# Introduction
With the ongoing pandemic I absorb statistics about the state of the virus, and to verify this information I went to HealthData.gov and found their Community Profile Report (CPR) – County-Level.  The following is some information on the dataset based on its documentation.  It was developed by Data Strategy and Execution Workgroup in the Joint Coordination Cell, under the White House COVID-19 Team. Each observation in the dataset is county-level.  It contains daily snapshot in time that focuses on recent COVID-19 outcomes in the last seven days and changes relative to the week prior.



---------------------------------------------------

# Package and Data import 

#### Packages
```{r}
library(tidyverse)
library(mvtnorm)
library(ggExtra)
library(rstatix)
library(sandwich)
library(lmtest)
library(interactions)
library(plotROC)
```


#### Data 
```{r}
FullCovid <- read_csv("COVID-19_Community_Profile_Report_-_County-Level.csv") 
CovidData <- FullCovid %>% 
  mutate( highCases = total_cases>median(total_cases,na.rm=T) ) %>% 
  mutate( region=recode(state, ME="NE", MA="NE", RI="NE", CT="NE", NH="NE", VT="NE", NY="NE", PA="NE", NJ="NE", DE="NE", MD="NE", DC="NE", PR="SE", WV="SE", VA="SE", KY="SE", TN="SE", NC="SE", SC="SE", GA="SE", AL="SE", MS="SE", AR="SE", LA="SE", FL="SE", OH="MW", IN="MW", MI="MW", IL="MW", MO="MW", WI="MW", MN="MW", IA="MW", KS="MW", NE="MW", SD="MW", ND="MW", TX="SW", OK="SW", NM="SW", AZ="SW", CO="W", WY="W", MT="W", ID="W", WA="W", OR="W", UT="W", NV="W", CA="W", AK="W", HI="W") ) %>% 
  select(fips, county, region, highCases, everything(), -state,-fema_region)
```


---------------------------------------------------

# MANOVA
## MANOVA
```{r}
man1<-manova(cbind(total_cases,total_deaths, cases_last_7_days, deaths_last_7_days, test_positivity_rate_last_7_days, confirmed_covid_hosp_last_7_days)~region, data=CovidData)
summary(man1)
```
*Total cases, total deaths, cases the last 7 days, deaths the last 7 days, test positivity rate the last 7 days, and confirmed covid hospitalizations show a mean difference across different regions of the US. *


### Univariate ANOVAs
```{r}
summary.aov(man1)
```
*Each of the afore mentioned predictor variables illustrate a mean difference across groups*

### Post-hoc t-tests
```{r}
#Total Cases
pairwise.t.test(CovidData$total_cases, CovidData$region, p.adj="none")$p.value*67 >0.05

#Total Deaths
pairwise.t.test(CovidData$total_deaths, CovidData$region, p.adj="none")$p.value *67 >0.05

#Cases in the Last 7 Days
pairwise.t.test(CovidData$cases_last_7_days, CovidData$region, p.adj="none")$p.value *67 >0.05

#Deaths int the Last 7 Days
pairwise.t.test(CovidData$deaths_last_7_days, CovidData$region, p.adj="none")$p.value *67 >0.05

#Test Positivity Rate in the Last 7 Days
pairwise.t.test(CovidData$test_positivity_rate_last_7_days, CovidData$region, p.adj="none")$p.value *67 >0.05

#Confirmed Covid Hospitalization in the last 7 Days
pairwise.t.test(CovidData$confirmed_covid_hosp_last_7_days, CovidData$region, p.adj="none")$p.value *67 >0.05
```
*The above matricies illustrate which group means significantly differ by region and by predictor variable (accounting for the change in significance level mentioned below)*

## Significance level(α)
```{r}
1-(0.95)^67

0.05/67
```
*With 6 numeric predictors and 5 catagorical groups, 67 inference tests were done.  This creates a 96.8% chance of having a Type-1 Error.  Thus a Bonferroni correction will be done to reduce the 0.05 signifcance level to 0.00075 (7.5e-4). *

## MANOVA assumptions
```{r}
group <- CovidData %>% na.omit() %>% select(region) %>% mutate(region = as.factor(region))
DVs <- CovidData %>% na.omit() %>% select(total_cases,total_deaths, cases_last_7_days, deaths_last_7_days, test_positivity_rate_last_7_days, confirmed_covid_hosp_last_7_days) 

#Test multivariate normality for each group (null: normality met)
sapply(split(DVs,group), mshapiro_test)

```
*The MANOVA fails the first assumption of normality, the other many other assumptions to check that are harder to meet such as Homogeneity of within-group covariance, Linear relationships among DVs, and the absense of outliers. *


---------------------------------------------------

# Randomization test
```{r}
randData <- CovidData %>% na.omit() %>% select(test_positivity_rate_last_7_days,cases_last_7_days)
sampleCor <- cor(randData$test_positivity_rate_last_7_days, randData$cases_last_7_days)

rand_dist<-vector()
for(i in 1:5000){
  new<-data.frame(positives=sample(randData$test_positivity_rate_last_7_days),cases=randData$cases_last_7_days) 
  rand_dist[i]<- cor(new$positives, new$cases)
}

ggplot( data.frame(rand_dist), aes(x=rand_dist) ) + 
  geom_histogram() +
  geom_vline(aes(xintercept=sampleCor), color="red")


pval <- mean(rand_dist>sampleCor | rand_dist < -sampleCor)
pval
```
*The Null Hypothesis is that there is no correlation between positive test rate in the past 7 days and the number of cases the past 7 days.  The null distribution of the correlation coefficient is illustrated in dark grey, and the in-sample correlation coefficient is the red vertical line.  The probability of a value as extreme as the in-sample value under this "randomization distribution" is 0, therefore we reject the null hypothesis that there is no correlation between these 2 variables. *

---------------------------------------------------

# Linear Regression Model:

### Creating and Interpreting
```{r}
lmData <- CovidData %>% 
  select( cases_last_7_days, test_positivity_rate_last_7_days, confirmed_covid_hosp_last_7_days ) %>% 
  mutate( posRate = test_positivity_rate_last_7_days-mean(test_positivity_rate_last_7_days,na.rm=T) ) %>% 
  mutate( confirmHosp = confirmed_covid_hosp_last_7_days-mean(CovidData$confirmed_covid_hosp_last_7_days,na.rm=T) )


fit<-lm(cases_last_7_days~posRate*confirmHosp, data=lmData)
summary(fit)
```
*Given average test positivity rate over the last 7 days and average confirmed covid hospitalizations over the last 7 days the predicted value of cases the last 7 days is 398.7656 cases*

*1197.4981 is the slope for test positivity rate over the last 7 days on cases the last 7 days while holding confirmed covid hospitalizations over the last 7 days constant*

*8.5943 is the slope for confirmed covid hospitalizations over the last 7 days on cases the last 7 days while holding test positivity rate over the last 7 days constant*

*The effect of positivity rate over the last 7 days is 10.5941 cases higher for every percent above the mean positivity rate over the last 7 days* 

*This linear model explain 81.42% of varience in cases the last 7 days *

### Plot of the Interaction 
```{r}
interact_plot(fit, confirmHosp, posRate, plot.points = T)
```


### Linear Model Assumptions
```{r}
#Normality
shapiro.test(fit$residuals) #H0: true distribution is normal

#Homoskedasticity
ggplot() + geom_point(aes(x=fit$fitted.values,y=fit$residuals))
bptest(fit) # H0: homoskedastic

```
*This linear model fails normality and homoscedasticity * 


### Robust standard errors
```{r}
coeftest(fit, vcov=vcovHC(fit))
```
*Because the model failed homoskeydacity robust SE were used  The robust SE changed the significance of the interaction, increased the positivity rate p-value,  but not above 0.05, and decreased all t values.* 



```{r}
samp_distn<-replicate(5000, {  
  boot_dat <- sample_frac(lmData, replace=T) #take bootstrap sample of rows  
  bootfit <- lm(cases_last_7_days~posRate*confirmHosp, data=boot_dat) #fit model on bootstrap sample  
  coef(bootfit) #save coefs
}) 

samp_distn %>% t %>% as.data.frame %>% summarize_all(sd)
```
*The bootstrapped SEs are very similar to robust SE and much higher than the original SE, illustrating the robust SE are a more accurate measure of the sample than than the original SE*



---------------------------------------------------

# Logistic Regression Models:

## Logistic Model with 2 predictors
### Creating and Interpreting
```{r}
fit2 <- glm(highCases~cases_last_7_days+total_deaths, data=CovidData, family="binomial")
summary(fit2)

exp(fit2$coefficients)
```
*Given the above model's coefficients: for every 1 unit increase in cases in the last 7 days the odds of highCases incrase by 1.01, and for every 1 unit increase in total deaths the odss of highCases increase by 1.02*


### Confusion Matrix
```{r}

CovidData <- CovidData %>% filter( complete.cases(CovidData$highCases),complete.cases(CovidData$cases_last_7_days),complete.cases(CovidData$total_deaths) )

table(Predicted = (predict(fit2,type="response")>0.5) , Actual = as.numeric(CovidData$highCases)) %>% addmargins

class_diag<-function(probs,truth){
  
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]

  if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
  
  #CALCULATE EXACT AUC
  ord<-order(probs, decreasing=TRUE)
  probs <- probs[ord]; truth <- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(acc,sens,spec,ppv,auc)
}

probs <- predict(fit2)

class_diag(probs, CovidData$highCases)
```
*See the above table and confusion matrix for diagnostic values of the logistic model with 2 predictors*

### Density Plot
```{r}

densityVal <- CovidData %>% mutate(logit=fit2$fitted.values )

ggplot(densityVal, aes(x=logit, fill=highCases)) + geom_density(alpha=.5)
```
*The above density plot illustrates the predictor value (logit) compared to the actual group membership to highCases*


### ROC and AUC
```{r}
ROCplot<-ggplot(CovidData)+geom_roc(aes(d=highCases,m=cases_last_7_days+total_deaths), n.cuts=0) 
ROCplot
calc_auc(ROCplot)

```
*The above ROC curve is of the logistic model with 2 predictors. Based on the shape of the ROC curve and the AUC of the aforementioned curve, the classification of this 2 predictor logistical model is very good. *

## Logistic Model with all predictors
### Creating and Interpreting
```{r}
lassoData <- CovidData %>% select(-fips, -county,-date) %>% na.omit
fit3 <- lm(highCases~(.), data=lassoData)
probs <- predict(fit3,data=CovidData)
class_diag(probs, lassoData$highCases)
```
*From including all variables the model was made more flexible, but had lower values for Accuracy, Sensitivity, and AUC, as there was more noise introduced. *



### 10 Fold Cross Validation
```{r}
k=10

cvData<-lassoData[sample(nrow(lassoData)),] #randomly order rows
folds<-cut(seq(1:nrow(lassoData)),breaks=k,labels=F) #create folds

diags<-NULL
for(i in 1:k){
  ## Create training and test sets  
  train<-cvData[folds!=i,]   
  test<-cvData[folds==i,]  
  truth<-test$highCases ## Truth labels for fold i
  
  ## Train model on training set (all but fold i)  
  lassofit<-glm(highCases~(.),data=train,family="binomial")
  
  ## Test model on test set (fold i)   
  probs<-predict(lassofit,newdata = test,type="response")
  
  ## Get diagnostics for fold i  
  diags<-rbind(diags,class_diag(probs,truth))
}

summarize_all(diags,mean) #average diagnostics across all k folds
```
*There is no sign of overfitting the more flexible model, with all predictors, as the AUC in 10 Fold Cross Validation was surprisingly higher for out-of-sample AUC than for the in-sample AUC. *


### LASSO
```{r}
library(glmnet)

y<-as.matrix(lassoData$highCases) #grab response
x<-model.matrix(highCases~(.),data=lassoData)[,-1] #grab predictors

cv<-cv.glmnet(x,y,family="binomial")
lasso<-glmnet(x,y,family="binomial",lambda=cv$lambda.1se)
coef(lasso)

newLassoData <- lassoData %>% mutate(regionNE=(region=="NE"))  %>% mutate(regionSW=(region=="SW")) %>% mutate(regionSE=(region=="SE"))

lassoSelectedVarFunction <- highCases~regionNE+
                  regionSE+
                  regionSW+
                  cases_per_100k_last_7_days+
                  total_cases+
                  cases_pct_change_from_prev_week+
                  deaths_per_100k_last_7_days+
                  deaths_pct_change_from_prev_week+
                  test_positivity_rate_last_7_days+
                  test_positivity_rate_pct_change_from_prev_week+
                  total_tests_pct_change_from_prev_week+
                  confirmed_covid_hosp_per_100_beds_last_7_days+
                  confirmed_covid_hosp_per_100_beds_pct_change_from_prev_week+
                  suspected_covid_hosp_last_7_days+
                  suspected_covid_hosp_per_100_beds_pct_change_from_prev_week+
                  pct_inpatient_beds_used_avg_last_7_days+
                  pct_inpatient_beds_used_covid_avg_last_7_days+
                  pct_inpatient_beds_used_covid_abs_change_from_prev_week+
                  pct_icu_beds_used_avg_last_7_days+
                  pct_icu_beds_used_abs_change_from_prev_week+
                  pct_icu_beds_used_covid_avg_last_7_days+
                  pct_icu_beds_used_covid_abs_change_from_prev_week+
                  pct_vents_used_avg_last_7_days+
                  pct_vents_used_covid_abs_change_from_prev_week

```
*The variables that were selected by LASSO were those that increased the ability of the model to predict highCases and did not add extra noise in the data for the model to learn.*

### CV with LASSO Vars
```{r}
######## IN-SAMPLE ########
fitLasso <- glm(lassoSelectedVarFunction, data=newLassoData)
probs <- predict(fitLasso)
class_diag(probs, newLassoData$highCases)


######## OUT-OF-SAMPLE ########
k=10
cvData<-newLassoData[sample(nrow(newLassoData)),] #randomly order rows
folds<-cut(seq(1:nrow(newLassoData)),breaks=k,labels=F) #create folds

diags<-NULL
for(i in 1:k){
  ## Create training and test sets  
  train<-cvData[folds!=i,]   
  test<-cvData[folds==i,]  
  truth<-test$highCases ## Truth labels for fold i
 
  lassofit<-glm(lassoSelectedVarFunction,data=train,family="binomial")  ## Train model on training set (all but fold i)  
  probs<-predict(lassofit,newdata = test,type="response") ## Test model on test set (fold i)   
  diags<-rbind(diags,class_diag(probs,truth))  ## Get diagnostics for fold i
}

summarize_all(diags,mean) #average diagnostics across all k folds
```
*Using the variables that LASSO selected, the model's out-of-sample AUC is higher than the model's in-sample AUC*
